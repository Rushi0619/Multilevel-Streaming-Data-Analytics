ğŸ“Š Multi-Level Real-Time Financial News Analytics using Kafka, Spark & Spark ML

ğŸ“Œ Project Overview
This project implements an end-to-end Big Data streaming analytics pipeline for financial news.
It ingests real-time news events using Apache Kafka, processes them using Apache Spark Structured Streaming, applies distributed machine learning (Spark ML) for sentiment prediction, and stores analytics results in AWS S3, enabling SQL-based querying via Amazon Athena.
The system follows a multi-level analytics architecture, gradually enriching data at each stage.

ğŸ—ï¸ Architecture Flow
Financial News CSV
        â†“
Kafka Producer
        â†“
Kafka Topic
        â†“
Level-1 Consumer (Python â€“ validation/filtering)
        â†“
Spark Structured Streaming (Level-2)
        â†“
Parquet Storage (Level-2 Output)
        â†“
Spark ML (TF-IDF + Logistic Regression)
        â†“
Rule-Based Financial Corrections
        â†“
Parquet Storage (Level-3 Output)
        â†“
AWS S3
        â†“
Amazon Athena (SQL Queries)


ğŸ§° Technology Stack
Layer
Technology
Messaging
Apache Kafka
Stream Processing
Apache Spark Structured Streaming
Machine Learning
Spark ML (Logistic Regression)
Storage
Parquet, AWS S3
Query Engine
Amazon Athena
Language
Python
Cloud
AWS EC2


ğŸ“ Actual Project Structure (Verified)
kafka-project/
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ financial_news_events.csv
â”‚
â”œâ”€â”€ producer.py
â”œâ”€â”€ consumer.py
â”œâ”€â”€ analytics_consumer.py
â”‚
â”œâ”€â”€ level2_spark_analytics.py
â”œâ”€â”€ level2_analytics_consumer.py
â”œâ”€â”€ check_level2.py
â”‚
â”œâ”€â”€ spark_ml_features.py
â”œâ”€â”€ spark_ml_train.py          â† Spark ML + rule-based correction logic
â”‚
â”œâ”€â”€ level2_output/
â”œâ”€â”€ level3_ml_predictions_fixed/
â”‚
â”œâ”€â”€ level3_storage_consumer.py
â”œâ”€â”€ level3_enriched_events.json
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ venv/


âš ï¸ Very Important Environment Note (READ THIS)
âŒ Why venv DOES NOT work for Spark ML
Spark ML internally depends on NumPy
Spark (3.5.x) still expects distutils
Python 3.12+ removed distutils
Result:
ModuleNotFoundError: No module named 'numpy'
ModuleNotFoundError: No module named 'distutils'


âœ… Solution Used in This Project
We created a separate Conda environment (sparkml) specifically for Spark ML execution.
ğŸ‘‰ Kafka + Spark Streaming â†’ venv
ğŸ‘‰ Spark ML â†’ conda (sparkml)
This separation is intentional and correct.

âš™ï¸ Prerequisites
EC2 Instance
Ubuntu 22.04 / 24.04
Minimum recommended:
4 vCPU
8â€“16 GB RAM
30+ GB storage
Software
Docker & Docker Compose
Java 11
Apache Spark 3.5.x
Python 3.x
Conda (Miniconda or Anaconda)
AWS CLI

ğŸš€ Step-by-Step Execution

ğŸ”¹ STEP 1: Start Kafka
docker-compose up -d
docker ps


ğŸ”¹ STEP 2: Python Environment for Kafka & Streaming
python3 -m venv venv
source venv/bin/activate
pip install kafka-python pandas pyspark


ğŸ”¹ STEP 3: Start Kafka Producer
python producer.py

Streams CSV data into Kafka topic.

ğŸ”¹ STEP 4: Level-2 Spark Streaming (Structured Streaming)
spark-submit level2_spark_analytics.py

âœ” Reads Kafka topic
âœ” Parses JSON
âœ” Stores structured Parquet data
Output:
level2_output/


ğŸ”¹ STEP 5: Verify Level-2 Output
spark-submit check_level2.py


ğŸ”¹ STEP 6: Create Conda Environment for Spark ML (CRITICAL)
conda create -n sparkml python=3.11 -y
conda activate sparkml
pip install numpy

ğŸ‘‰ All Spark ML steps must be run inside this environment

ğŸ”¹ STEP 7: Spark ML Feature Engineering
spark-submit spark_ml_features.py

Creates:
Tokenized text
Stop-word removal
TF-IDF feature vectors

ğŸ”¹ STEP 8: Spark ML Training + Rule-Based Corrections
spark-submit spark_ml_train.py

What happens inside spark_ml_train.py:
Loads Level-2 Parquet
Applies rule-based financial corrections
(e.g., â€œbreachâ€, â€œcollapseâ€ â†’ Negative)
Encodes labels using StringIndexer
Trains Logistic Regression
Generates predictions & probabilities
Stores results as Parquet
Output:
level3_ml_predictions_fixed/


ğŸ”¹ STEP 9: Upload Level-3 Output to S3
aws s3 cp level3_ml_predictions_fixed/ \
s3://<your-bucket-name>/level3_ml_predictions/ \
--recursive


ğŸ”¹ STEP 10: Query via Amazon Athena
CREATE DATABASE financial_news_db;

CREATE EXTERNAL TABLE financial_news_db.sentiment_predictions (
  Headline STRING,
  Sentiment STRING,
  prediction DOUBLE,
  probability ARRAY<DOUBLE>
)
STORED AS PARQUET
LOCATION 's3://<your-bucket-name>/level3_ml_predictions/';

SELECT * FROM financial_news_db.sentiment_predictions LIMIT 10;


ğŸ§  Machine Learning Summary
Algorithm: Logistic Regression (Spark ML)
Features: TF-IDF
Classes: Positive, Negative, Neutral
Enhancement: Rule-based sentiment correction
Reason: Scalable, explainable, production-friendly

ğŸ“ˆ Key Learnings
Kafka + Spark Streaming integration
Multi-level analytics architecture
Distributed ML using Spark ML
Cloud storage & SQL analytics
Environment isolation for Spark ML stability

ğŸ”® Future Scope
Replace Logistic Regression with LSTM (offline training)
Real-time Spark ML inference from Kafka
QuickSight dashboards
Model versioning & evaluation metrics

ğŸ‘¤ Author
Rushikesh Ashok Ghotkar
B.E. â€“ Artificial Intelligence & Data Science
